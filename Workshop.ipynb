{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import gym\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def send_score(reward):\n",
    "    my_id = 'rl_user_1'\n",
    "    name = 'My Name goes Here'\n",
    "    image = 'image_url'\n",
    "    try:\n",
    "        requests.post(\"http://workshop.sauray.com/score\", data={'id': my_id, 'name': name, 'score': reward, 'image': image})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "NUMBER_OF_OBSERVATIONS=8\n",
    "NUMBER_OF_ACTIONS=4\n",
    "NUMBER_OF_GAMES_TO_PLAY=400\n",
    "NUMBER_OF_STEPS=40000\n",
    "MAX_MEMORY_LENGTH = 100000\n",
    "NUMBER_INITIAL_OBSERVATIONS = 0\n",
    "TRAIN_EVERY_N_GAMES=50\n",
    "TRAIN = True\n",
    "\n",
    "# One hot encoding array https://fr.wikipedia.org/wiki/Encodage_one-hot\n",
    "possible_actions = np.arange(0,NUMBER_OF_ACTIONS)\n",
    "actions_one_hot_encoding = np.zeros((NUMBER_OF_ACTIONS,NUMBER_OF_ACTIONS))\n",
    "actions_one_hot_encoding[np.arange(NUMBER_OF_ACTIONS),possible_actions] = 1\n",
    "\n",
    "# Create enviroment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# TODO\n",
    "# Define the function which returns the model, i.e the function approximator\n",
    "# You can find information here https://keras.io/models/sequential/\n",
    "def model():\n",
    "    return LinearRegression()\n",
    "\n",
    "def load_model(m, filename='model.pkl'):\n",
    "    return joblib.load(filename) \n",
    "\n",
    "def save_model(m, filename='model.pkl'):\n",
    "    joblib.dump(m, filename) \n",
    "\n",
    "# TODO\n",
    "# The probability under which a random action is performed\n",
    "# Try to find the balance between exploration and exploitation\n",
    "def get_epsilon(game_no, game_max_no):\n",
    "    return 0.5\n",
    "\n",
    "# The epsilon greedy policy\n",
    "# This function should return an action (0, 1, 2 or 3)\n",
    "# It should call the get_epsilon function\n",
    "# Under an epsilon probability, pick a random action\n",
    "# Otherwise, find the best action according to your model\n",
    "# You should predict the value of the vectors\n",
    "# [observation1, observation2, ... , action0] [observation1, observation2, ... , action1] etc up to the number of actions (4)\n",
    "def epsilon_greedy(env, observation, approximator, game):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Memory Array data array \n",
    "memoryX = np.zeros(shape=(1,NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "memoryY = np.zeros(shape=(1,1))\n",
    "\n",
    "try:\n",
    "    approximator = load_model()\n",
    "except:\n",
    "    print(\"load failed\")\n",
    "    approximator = model()\n",
    "\n",
    "def calculate_q_values(rewards, b_discount):\n",
    "    rewards_length = len(rewards)\n",
    "    for i in range(0, rewards_length):\n",
    "        # compte the discounted reward and set the rewards\n",
    "        index = (rewards_length-1)-i # this is the reverse index, we start from end to beginning\n",
    "        if i == 0:\n",
    "            # avoid crashing if i==0 (remember, we are iterating reversly)\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "    return rewards\n",
    "\n",
    "def update_memory(memoryX, memoryY, X, y):\n",
    "    if memoryX.shape[0] == 1:\n",
    "        memoryX = X\n",
    "        memoryY = y\n",
    "    else:\n",
    "        #Add experience to memory\n",
    "        memoryX = np.concatenate((memoryX, X),axis=0)\n",
    "        memoryY = np.concatenate((memoryY, y),axis=0)\n",
    "        # if memory is full remove first element\n",
    "        if np.alen(memoryX) >= MAX_MEMORY_LENGTH:\n",
    "            for l in range(np.alen(X)):\n",
    "                memoryX = np.delete(memoryX, 0, axis=0)\n",
    "                memoryY = np.delete(memoryY, 0, axis=0)\n",
    "    return memoryX, memoryY\n",
    "\n",
    "game_won = 0\n",
    "game_lost = 0\n",
    "for game in range(NUMBER_OF_GAMES_TO_PLAY):\n",
    "    # the vector that combines the environment and the action\n",
    "    X = np.zeros(shape=(1,NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "    y = np.zeros(shape=(1,1))\n",
    "    # reset the environment to start a new game\n",
    "    qs = env.reset()\n",
    "    sum_rewards = 0\n",
    "    for step in range (NUMBER_OF_STEPS):\n",
    "\n",
    "        action = epsilon_greedy(env, qs, approximator, game)\n",
    "        env.render()\n",
    "        qs_a = np.concatenate((qs, actions_one_hot_encoding[action]), axis=0)        \n",
    "        observation,reward,done,info = env.step(action)\n",
    "        sum_rewards = sum_rewards + reward\n",
    "        if step == 0:\n",
    "            X[0] = qs_a\n",
    "            y = np.array([reward])\n",
    "            memoryX[0] = qs_a\n",
    "            memoryY[0] = np.array([reward])\n",
    "        X = np.vstack((X,qs_a))\n",
    "        y = np.vstack((y, np.array([reward])))\n",
    "        \n",
    "        if done:\n",
    "            # calculate Q values from end to start, using the Bellman equation\n",
    "            # You need to find a good parameter for b_discount (look for the Bellman equation)\n",
    "            calculate_q_values(y, b_discount=0.98)\n",
    "            (memoryX, memoryY) = update_memory(memoryX, memoryY, X, y)\n",
    "        \n",
    "        # Update the states\n",
    "        qs=observation\n",
    "        # Train every X game after num_initial_observation\n",
    "        if done:\n",
    "            if TRAIN and game >= NUMBER_INITIAL_OBSERVATIONS and game%TRAIN_EVERY_N_GAMES == 0 and game >= TRAIN_EVERY_N_GAMES:\n",
    "                print(\"Training  game# \", game,\"memory size\", memoryX.shape[0])\n",
    "                approximator.fit(memoryX,memoryY)\n",
    "                np.save('memoryX.csv', memoryX)\n",
    "                np.save('memoryY.csv', memoryY)\n",
    "                save_model(approximator)\n",
    "                env.reset()\n",
    "            send_score(sum_rewards)\n",
    "            if reward >= 0 and reward <99:\n",
    "                print(\"Game \",game,\" ended with positive reward \")\n",
    "            if reward > 50:\n",
    "                game_won = game_won + 1\n",
    "                print(\"Game \", game,\" WON *** with reward \"+str(reward) )\n",
    "            else:\n",
    "                game_lost = game_lost + 1\n",
    "                print(\"Game \", game,\" FAILED *** with reward \"+str(reward))\n",
    "            win_ratio = game_won/(game_won+game_lost)\n",
    "            print(\"Success: \" + str(win_ratio*100) +\"%\")\n",
    "            break\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
