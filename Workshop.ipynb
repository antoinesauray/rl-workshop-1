{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2018-01-18 15:21:15,471] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00637188,  0.94257537,  0.64538345,  0.1307813 , -0.00737658,\n",
       "       -0.146189  ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import gym\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def send_score(reward):\n",
    "    my_id = 'rl_user_1'\n",
    "    name = 'My Name goes Here'\n",
    "    image = 'image_url'\n",
    "    try:\n",
    "        requests.post(\"http://workshop.sauray.com/score\", data={'id': my_id, 'name': name, 'score': reward, 'image': image})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "NUMBER_OF_OBSERVATIONS=8\n",
    "NUMBER_OF_ACTIONS=4\n",
    "NUMBER_OF_GAMES_TO_PLAY=400\n",
    "NUMBER_OF_STEPS=40000\n",
    "MAX_MEMORY_LENGTH = 100000\n",
    "NUMBER_INITIAL_OBSERVATIONS = 0\n",
    "TRAIN_EVERY_N_GAMES=50\n",
    "TRAIN = True\n",
    "\n",
    "# One hot encoding array https://fr.wikipedia.org/wiki/Encodage_one-hot\n",
    "possible_actions = np.arange(0,NUMBER_OF_ACTIONS)\n",
    "actions_one_hot_encoding = np.zeros((NUMBER_OF_ACTIONS,NUMBER_OF_ACTIONS))\n",
    "actions_one_hot_encoding[np.arange(NUMBER_OF_ACTIONS),possible_actions] = 1\n",
    "\n",
    "# Create enviroment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "\n",
    "# TODO\n",
    "# Define the function which returns the model, i.e the function approximator\n",
    "# You can find information here https://keras.io/models/sequential/\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation='relu', input_dim=NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def load_model(filename=\"solution-weights.h5\"):\n",
    "    m = model()\n",
    "    m.load_weights(filename)\n",
    "    return m\n",
    "\n",
    "def save_model(m, filename=\"solution-weights.h5\"):\n",
    "    m.save_weights(filename)\n",
    "\n",
    "# TODO\n",
    "# The probability under which a random action is performed\n",
    "# Try to find the balance between exploration and exploitation\n",
    "def get_epsilon(game_no, game_max_no):\n",
    "    return 0.5\n",
    "\n",
    "# The epsilon greedy policy\n",
    "# This function should return an action (0, 1, 2 or 3)\n",
    "# It should call the get_epsilon function\n",
    "# Under an epsilon probability, pick a random action\n",
    "# Otherwise, find the best action according to your model\n",
    "# You should predict the value of the vectors\n",
    "# [observation1, observation2, ... , action0] [observation1, observation2, ... , action1] etc up to the number of actions (4)\n",
    "def epsilon_greedy(env, observation, approximator, game):\n",
    "    starting_probability = 0.0\n",
    "    explore_prob = starting_probability - (starting_probability/NUMBER_OF_GAMES_TO_PLAY)*game\n",
    "    #explore_prob = 0.35\n",
    "    if np.random.rand(1)<explore_prob:\n",
    "        # random\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        x = np.zeros(shape=(0,NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "        for action_vector in actions_one_hot_encoding:\n",
    "            v = np.concatenate((observation, action_vector))\n",
    "            x = np.vstack((x, v))\n",
    "        predictions = approximator.predict(x)\n",
    "        return np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load failed\n",
      "Game  0  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  1  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  2  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  3  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  4  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  5  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  6  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  7  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  8  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  9  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  10  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  11  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  12  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  13  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  14  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  15  FAILED *** with reward -100\n",
      "Success: 0.0%\n",
      "Game  16  FAILED *** with reward -100\n",
      "Success: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-18 15:21:37,480] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-783a127cbfc9>\", line 47, in <module>\n",
      "    action = epsilon_greedy(env, qs, approximator, game)\n",
      "  File \"<ipython-input-2-d6073607d0fe>\", line 52, in epsilon_greedy\n",
      "    predictions = approximator.predict(x)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/keras/models.py\", line 1006, in predict\n",
      "    return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/keras/engine/training.py\", line 1790, in predict\n",
      "    verbose=verbose, steps=steps)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/keras/engine/training.py\", line 1299, in _predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1089, in _run\n",
      "    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/inspect.py\", line 1454, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/inspect.py\", line 671, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/inspect.py\", line 717, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/posixpath.py\", line 373, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/posixpath.py\", line 407, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/antoinesauray/anaconda/envs/idp/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Initialize Memory Array data array \n",
    "memoryX = np.zeros(shape=(1,NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "memoryY = np.zeros(shape=(1,1))\n",
    "\n",
    "try:\n",
    "    approximator = load_model()\n",
    "except:\n",
    "    print(\"load failed\")\n",
    "    approximator = model()\n",
    "\n",
    "def calculate_q_values(rewards, b_discount):\n",
    "    rewards_length = len(rewards)\n",
    "    for i in range(0, rewards_length):\n",
    "        index = (rewards_length-1)-i\n",
    "        if i==0:\n",
    "            rewards[index] = (rewards[index])\n",
    "        else:\n",
    "            rewards[index] = rewards[index]+b_discount*rewards[index+1]\n",
    "    return rewards\n",
    "\n",
    "def update_memory(memoryX, memoryY, X, y):\n",
    "    if memoryX.shape[0] == 1:\n",
    "        memoryX = X\n",
    "        memoryY = y\n",
    "    else:\n",
    "        #Add experience to memory\n",
    "        memoryX = np.concatenate((memoryX, X),axis=0)\n",
    "        memoryY = np.concatenate((memoryY, y),axis=0)\n",
    "        # if memory is full remove first element\n",
    "        if np.alen(memoryX) >= MAX_MEMORY_LENGTH:\n",
    "            for l in range(np.alen(X)):\n",
    "                memoryX = np.delete(memoryX, 0, axis=0)\n",
    "                memoryY = np.delete(memoryY, 0, axis=0)\n",
    "    return memoryX, memoryY\n",
    "\n",
    "game_won = 0\n",
    "game_lost = 0\n",
    "for game in range(NUMBER_OF_GAMES_TO_PLAY):\n",
    "    # the vector that combines the environment and the action\n",
    "    X = np.zeros(shape=(1,NUMBER_OF_OBSERVATIONS+NUMBER_OF_ACTIONS))\n",
    "    y = np.zeros(shape=(1,1))\n",
    "    # reset the environment to start a new game\n",
    "    qs = env.reset()\n",
    "    sum_rewards = 0\n",
    "    for step in range (NUMBER_OF_STEPS):\n",
    "\n",
    "        action = epsilon_greedy(env, qs, approximator, game)\n",
    "        env.render()\n",
    "        qs_a = np.concatenate((qs, actions_one_hot_encoding[action]), axis=0)        \n",
    "        observation,reward,done,info = env.step(action)\n",
    "        sum_rewards = sum_rewards + reward\n",
    "        if step == 0:\n",
    "            X[0] = qs_a\n",
    "            y = np.array([reward])\n",
    "            memoryX[0] = qs_a\n",
    "            memoryY[0] = np.array([reward])\n",
    "        X = np.vstack((X,qs_a))\n",
    "        y = np.vstack((y, np.array([reward])))\n",
    "        \n",
    "        if done:\n",
    "            # calculate Q values from end to start, using the Bellman equation\n",
    "            # You need to find a good parameter for b_discount (look for the Bellman equation)\n",
    "            calculate_q_values(y, b_discount=0.98)\n",
    "            (memoryX, memoryY) = update_memory(memoryX, memoryY, X, y)\n",
    "        \n",
    "        # Update the states\n",
    "        qs=observation\n",
    "        # Train every X game after num_initial_observation\n",
    "        if done:\n",
    "            if TRAIN and game >= NUMBER_INITIAL_OBSERVATIONS and game%TRAIN_EVERY_N_GAMES == 0 and game >= TRAIN_EVERY_N_GAMES:\n",
    "                print(\"Training  game# \", game,\"memory size\", memoryX.shape[0])\n",
    "                approximator.fit(memoryX,memoryY)\n",
    "                np.save('memoryX.csv', memoryX)\n",
    "                np.save('memoryY.csv', memoryY)\n",
    "                save_model(approximator)\n",
    "                env.reset()\n",
    "            send_score(sum_rewards)\n",
    "            if reward >= 0 and reward <99:\n",
    "                print(\"Game \",game,\" ended with positive reward \")\n",
    "            if reward > 50:\n",
    "                game_won = game_won + 1\n",
    "                print(\"Game \", game,\" WON *** with reward \"+str(reward) )\n",
    "            else:\n",
    "                game_lost = game_lost + 1\n",
    "                print(\"Game \", game,\" FAILED *** with reward \"+str(reward))\n",
    "            win_ratio = game_won/(game_won+game_lost)\n",
    "            print(\"Success: \" + str(win_ratio*100) +\"%\")\n",
    "            break\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
